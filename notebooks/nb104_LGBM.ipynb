{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPS-Feb-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = '104'\n",
    "dataset_NB = '002'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libralies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode\n",
    "import time\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier, early_stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "##### Load train and Test set\n",
    "train = pd.read_pickle(f\"../data/processed/nb{dataset_NB}_train.pkl\", compression='zip')\n",
    "test = pd.read_pickle(f\"../data/processed/nb{dataset_NB}_test.pkl\", compression='zip')\n",
    "\n",
    "submission = pd.read_csv('../data/raw/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train)\n",
    "test_id = test['row_id']\n",
    "\n",
    "#dataset = pd.concat(objs=[train, test], axis=0).reset_index(drop=True)\n",
    "# reset_index: indexを0から順に振り直す\n",
    "# drop: Falseの場合、元のindexが「index」列が新たに生成されて残る。Trueの場合「index」列は作られない。\n",
    "\n",
    "#dataset = dataset.drop(columns=['row_id'])\n",
    "#train = train.drop(columns=['row_id'])\n",
    "\n",
    "#dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 13\n",
    "FOLDS = 4\n",
    "TARGET = 'target'\n",
    "FEATURES = [col for col in train.columns if col not in ['row_id', TARGET]]\n",
    "\n",
    "FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目的変数（target）を数値に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "train[TARGET] = encoder.fit_transform(train[TARGET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_fixed_params = {\n",
    "    'objective' : 'multiclass',\n",
    "    'metric' : 'multi_logloss',\n",
    "    \"n_estimators\": 3000,\n",
    "    'learning_rate': 0.1,\n",
    "}\n",
    "\n",
    "#fit_params = {\"early_stopping_rounds\": 100,\n",
    "#            \"eval_set\": [[X_test, y_test]]}\n",
    "\n",
    "\n",
    "# LightGBM parameters\n",
    "lgb_param_grid = {\n",
    "                 #'learning_rate': [0.1, 0.05],\n",
    "                 'num_leaves' : [31, 63],\n",
    "                 #'num_leaves' : [12, 15, 18],\n",
    "                 'max_depth'  : [15, 21],\n",
    "                 #'max_depth'  : [6, 9, 15],\n",
    "                 #'min_gain_to_split' : [0, 0.1, 0.2],\n",
    "                 #'feature_fraction' : [0.5, 0.7, 1],\n",
    "                 #'bagging_fraction' : [0.7, 0.9, 1],\n",
    "                 #'min_sum_hessian_in_leaf' : [1, 2, 4],\n",
    "                }\n",
    "\n",
    "callbacks = [early_stopping(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_predictions = []\n",
    "lgb_scores = []\n",
    "lgb_feature_importance = []\n",
    "lgb_result = {}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train[FEATURES], train[TARGET])):\n",
    "\n",
    "    print(10*\"=\", f\"Fold={fold+1}\", 10*\"=\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    ### データセット作成\n",
    "    X_train, X_val = train.iloc[train_idx][FEATURES], train.iloc[val_idx][FEATURES]\n",
    "    y_train, y_val = train[TARGET].iloc[train_idx] , train[TARGET].iloc[val_idx]\n",
    "\n",
    "    ### 学習\n",
    "    lgb_model = LGBMClassifier(**lgb_fixed_params)\n",
    "\n",
    "    # gs = GridSearchCV(model, param_grid=lgb_param_grid, fit_params=lgb_fixed_params, cv=FOLDS, n_jobs=-1, verbose=2)\n",
    "    gs_lgb = GridSearchCV(lgb_model, param_grid=lgb_param_grid, cv=FOLDS, n_jobs=-1, verbose=2)\n",
    "    gs_lgb.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=0, callbacks=callbacks)\n",
    "\n",
    "    #model.fit(X_train, y_train,verbose=0)\n",
    "    #model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=0, callbacks=callbacks)\n",
    "\n",
    "    ### 結果格納\n",
    "    lgb_result[f'Fold{fold}_best_estimator'] = gs_lgb.best_estimator_\n",
    "    lgb_result[f'Fold{fold}_best_score'] = gs_lgb.best_score_\n",
    "    lgb_result[f'Fold{fold}_best_grid_params'] = gs_lgb.best_params_\n",
    "    lgb_result[f'Fold{fold}_best_all_params'] = lgb_result[f'Fold{fold}_best_estimator'].get_params()\n",
    "    lgb_result[f'Fold{fold}_cv_result'] = pd.DataFrame(gs_lgb.cv_results_)\n",
    "\n",
    "    ### Best Score\n",
    "    print(f\"Best Score: {gs_lgb.best_score_}\")\n",
    "    print(f\"Best Param: {gs_lgb.best_params_}\")\n",
    "\n",
    "    ### 推論（validation）\n",
    "    preds_val = gs_lgb.predict(X_val)\n",
    "    acc = accuracy_score(y_val, preds_val)\n",
    "    lgb_scores.append(acc)\n",
    "\n",
    "    ### 結果格納\n",
    "    lgb_result[f'Fold{fold}_preds_val'] = preds_val\n",
    "    lgb_result[f'Fold{fold}_y_val'] = y_val\n",
    "    lgb_result[f'Fold{fold}_acc'] = acc\n",
    "\n",
    "    ### feature importance\n",
    "    feat_imp = pd.DataFrame(index=FEATURES, data=lgb_result[f'Fold{fold}_best_estimator'].feature_importances_, columns=[f'{fold}_importance'])\n",
    "    lgb_feature_importance.append(feat_imp)\n",
    "\n",
    "    ### 推論（test）\n",
    "    test_preds = gs_lgb.predict(test[FEATURES])\n",
    "    lgb_predictions.append(test_preds)\n",
    "\n",
    "    ### 結果表示\n",
    "    run_time = time.time() - start_time\n",
    "    print(f\"Fold={fold+1}, Accuracy: {acc:.5f}, Run Time: {run_time:.2f}s\")\n",
    "\n",
    "print(10*\"=\", \"Cross Validation finished.\", 10*\"=\")\n",
    "print(\"Mean Accuracy :\", np.mean(lgb_scores))\n",
    "print(lgb_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_fis_df = pd.concat(lgb_feature_importance, axis=1).head(15)\n",
    "lgbm_fis_df.sort_values('1_importance').plot(kind='barh', figsize=(15, 10), title='Feature Importance Across Folds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_submission = submission.copy()\n",
    "lgb_submission[TARGET] = encoder.inverse_transform(np.squeeze(mode(np.column_stack(lgb_predictions),axis = 1)[0]).astype('int'))\n",
    "#lgb_submission[TARGET] = np.squeeze(mode(np.column_stack(lgb_predictions),axis = 1)[0]).astype('int')\n",
    "### 列方向にリストを行列化して、各行の最頻値をとって、１次元の配列は削除して、intにして、数値をラベルに戻している\n",
    "\n",
    "lgb_submission.to_csv(f\"../data/submission/nb{NB}_LGBM.csv\",index=False)\n",
    "lgb_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検証メモ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params = {\"early_stopping_rounds\": 100,\n",
    "              \"eval_set\": [[X_test, y_test]]}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "gs = GridSearchCV(xgb_model,\n",
    "                  params,\n",
    "                  fit_params=fit_params,\n",
    "                  cv=10,\n",
    "                  n_jobs=-1,\n",
    "                  verbose=2)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "def GridSearchCV_XGB_early_stoppping(param_grid, param_fixed, scorer, cv, X, y):\n",
    "    \"\"\"This function performs grid search for the best set of parameters of XGBoost model with early stopping.\n",
    "\n",
    "    Args:\n",
    "        param_grid (dict): The parameter ranges for which the function searches.\n",
    "        param_fixed (dict): The fitting parameters for XGBoost.\n",
    "        scorer (_PredictScorer): The sklearn's scorer instance.\n",
    "        cv (model_selection._split): The sklearn's split instance.\n",
    "        X (DataFrame): The input data matrix.\n",
    "        y (Series): The ground truth label.\n",
    "\n",
    "    Returns:\n",
    "        dict: The best set of parameters found via grid search.\n",
    "    \"\"\"\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.to_numpy()\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.to_numpy()\n",
    "\n",
    "    param_names, param_values = zip(*list(param_grid.items()))\n",
    "\n",
    "    cv_best_iterations = defaultdict(list)\n",
    "    cv_results = defaultdict(list)\n",
    "\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        gscv_x_train, gscv_x_val = X[train_index], X[test_index]\n",
    "        gscv_y_train, gscv_y_val = y[train_index], y[test_index]\n",
    "\n",
    "        param_fixed_cv = deepcopy(param_fixed)\n",
    "        param_fixed_cv['eval_set'] = [(gscv_x_val, gscv_y_val)]\n",
    "\n",
    "        for value_combination in product(*param_values):\n",
    "            param_grid_cv = tuple(zip(param_names, value_combination))\n",
    "            xgboost = XGBRegressor(**dict(param_grid_cv))\n",
    "\n",
    "            xgboost.fit(gscv_x_train, gscv_y_train, **param_fixed_cv)\n",
    "            if 'early_stopping_rounds' not in param_fixed_cv:\n",
    "                best_iteration = xgboost.get_num_boosting_rounds()\n",
    "            else:\n",
    "                best_iteration = xgboost.best_iteration\n",
    "            cv_best_iterations[param_grid_cv].append(best_iteration)\n",
    "\n",
    "            score = scorer(xgboost, gscv_x_val, gscv_y_val)\n",
    "            cv_results[param_grid_cv].append(score)\n",
    "\n",
    "    best_params_xgb, score_list = max(cv_results.items(), key=lambda x: np.array(x[1]).mean())\n",
    "\n",
    "    # Note that our XGBoost model may stop early,\n",
    "    # so we calculate the mean of the actual number of estimators in each fold,\n",
    "    # in place of the originally planned n_estimators after finishing cross validation.\n",
    "    n_estimators = int(round(np.array(cv_best_iterations[best_params_xgb]).mean()))\n",
    "\n",
    "    best_params_xgb = dict(best_params_xgb)\n",
    "    best_params_xgb['n_estimators'] = n_estimators\n",
    "\n",
    "    print (\"Best score: {:.3f}\".format(np.array(score_list).mean()))\n",
    "    print (\"Best Parameters: {}\".format(best_params_xgb))\n",
    "\n",
    "    return best_params_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc8a78a13283e3ba74119858067a74c2c7a55702e09c935fdd8fe4b244251524"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('3.9.7': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
